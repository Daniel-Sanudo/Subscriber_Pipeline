{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ddcd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 29 7125k   29 2109k    0     0  4426k      0  0:00:01 --:--:--  0:00:01 4423k\n",
      "100 7125k  100 7125k    0     0  7595k      0 --:--:-- --:--:-- --:--:-- 7596k\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "!curl -O https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.34.0/sqlite-jdbc-3.34.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b142196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, DateType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e883946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create connection to the database file and the cursor to manage it \n",
    "con = sqlite3.connect(os.path.join('dev','cademycode.db'))\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b545e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cademycode_students',), ('cademycode_courses',), ('cademycode_student_jobs',)]\n"
     ]
    }
   ],
   "source": [
    "# Store the table names from the database\n",
    "table_names = cur.execute('''SELECT name FROM sqlite_master''').fetchall()\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21d216e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('EDA') \\\n",
    "        .master('local[*]') \\\n",
    "        .config(\n",
    "        \"spark.jars\",\n",
    "        \"{}/sqlite-jdbc-3.34.0.jar\".format(os.getcwd())) \\\n",
    "        .config(\n",
    "        \"spark.driver.extraClassPath\",\n",
    "        \"{}/sqlite-jdbc-3.34.0.jar\".format(os.getcwd())) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e5f98b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+---+--------------------+------+-------------+--------------+-----------+\n",
      "|uuid|                name| birthdate|sex|        contact_info|job_id|courses_count|career_path_id|hours_spent|\n",
      "+----+--------------------+----------+---+--------------------+------+-------------+--------------+-----------+\n",
      "|   1|     Annabelle Avery|1943-07-03|  F|{\"mailing_address...|   7.0|          6.0|           1.0|       4.99|\n",
      "|   2|         Micah Rubio|1991-02-07|  M|{\"mailing_address...|   7.0|          5.0|           8.0|        4.4|\n",
      "|   3|          Hosea Dale|1989-12-07|  M|{\"mailing_address...|   7.0|          8.0|           8.0|       6.74|\n",
      "|   4|        Mariann Kirk|1988-07-31|  F|{\"mailing_address...|   6.0|          7.0|           9.0|      12.31|\n",
      "|   5|     Lucio Alexander|1963-08-31|  M|{\"mailing_address...|   7.0|         14.0|           3.0|       5.64|\n",
      "|   6|    Shavonda Mcmahon|1989-10-15|  F|{\"mailing_address...|   6.0|         10.0|           3.0|      10.12|\n",
      "|   7| Terrell Bleijenberg|1959-05-05|  M|{\"mailing_address...|   2.0|          9.0|           8.0|      24.17|\n",
      "|   8|      Stanford Allan|1997-11-22|  M|{\"mailing_address...|   3.0|          3.0|           1.0|      19.54|\n",
      "|   9|     Tricia Delacruz|1961-10-20|  F|{\"mailing_address...|   1.0|          6.0|           9.0|       1.75|\n",
      "|  10|Regenia van der Helm|1999-02-23|  N|{\"mailing_address...|   5.0|          7.0|           6.0|      13.55|\n",
      "|  11|    Shonda Stephanin|1998-10-24|  F|{\"mailing_address...|   7.0|          5.0|          10.0|      12.61|\n",
      "|  12|    Marcus Mcfarland|1977-05-29|  M|{\"mailing_address...|   7.0|          3.0|           6.0|       9.12|\n",
      "|  13|   Edwardo Boonzayer|1975-05-23|  N|{\"mailing_address...|   2.0|         15.0|           5.0|       6.09|\n",
      "|  14|      Robena Padilla|1969-01-15|  F|{\"mailing_address...|   3.0|          5.0|           9.0|      15.92|\n",
      "|  15|        Tamala Sears|1942-06-01|  F|{\"mailing_address...|   7.0|         13.0|           1.0|       4.64|\n",
      "|  16|       Norene Dalton|1976-04-30|  F|{\"mailing_address...|   6.0|          0.0|          null|       null|\n",
      "|  17|      Maris Benskoop|1965-08-10|  F|{\"mailing_address...|   5.0|          5.0|           4.0|      12.08|\n",
      "|  18|    Yolande van Hees|1978-12-09|  F|{\"mailing_address...|   7.0|         13.0|           9.0|      34.44|\n",
      "|  19|      Dominic Werner|1973-09-29|  M|{\"mailing_address...|   5.0|         14.0|           8.0|      22.84|\n",
      "|  20|Sofia van Steenbe...|1990-02-21|  N|{\"mailing_address...|   7.0|         13.0|          null|       null|\n",
      "+----+--------------------+----------+---+--------------------+------+-------------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "[('uuid', 'bigint'), ('name', 'string'), ('birthdate', 'string'), ('sex', 'string'), ('contact_info', 'string'), ('job_id', 'string'), ('courses_count', 'string'), ('career_path_id', 'string'), ('hours_spent', 'string')]\n"
     ]
    }
   ],
   "source": [
    "# Create list with column names\n",
    "student_columns = ['uuid', 'name', 'birthdate', \n",
    "                   'sex', 'contact_info', 'job_id', \n",
    "                   'courses_count', 'career_path_id', 'hours_spent']\n",
    "\n",
    "# Create pyspark dataframe object using the queried information\n",
    "student_df = spark.createDataFrame(cur.execute('''SELECT * FROM cademycode_students''').fetchall(),student_columns)\n",
    "student_df.show()\n",
    "\n",
    "# Show the dtypes which were assigned\n",
    "print(student_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f631f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------------+\n",
      "|career_path_id|    career_path_name|hours_to_complete|\n",
      "+--------------+--------------------+-----------------+\n",
      "|             1|      data scientist|               20|\n",
      "|             2|       data engineer|               20|\n",
      "|             3|        data analyst|               12|\n",
      "|             4|software engineering|               25|\n",
      "|             5|    backend engineer|               18|\n",
      "|             6|   frontend engineer|               20|\n",
      "|             7|       iOS developer|               27|\n",
      "|             8|   android developer|               27|\n",
      "|             9|machine learning ...|               35|\n",
      "|            10|      ux/ui designer|               15|\n",
      "+--------------+--------------------+-----------------+\n",
      "\n",
      "[('career_path_id', 'bigint'), ('career_path_name', 'string'), ('hours_to_complete', 'bigint')]\n"
     ]
    }
   ],
   "source": [
    "# Create list with column names\n",
    "course_columns = ['career_path_id', 'career_path_name', 'hours_to_complete']\n",
    "\n",
    "# Create pyspark dataframe using the data from the query\n",
    "course_df = spark.createDataFrame(cur.execute('''SELECT * FROM cademycode_courses''').fetchall(),course_columns)\n",
    "course_df.show()\n",
    "\n",
    "print(course_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b94ba4bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+----------+\n",
      "|job_id|      job_category|avg_salary|\n",
      "+------+------------------+----------+\n",
      "|     1|         analytics|     86000|\n",
      "|     2|          engineer|    101000|\n",
      "|     3|software developer|    110000|\n",
      "|     4|          creative|     66000|\n",
      "|     5|financial services|    135000|\n",
      "|     6|         education|     61000|\n",
      "|     7|                HR|     80000|\n",
      "|     8|           student|     10000|\n",
      "|     9|        healthcare|    120000|\n",
      "|     0|             other|     80000|\n",
      "|     3|software developer|    110000|\n",
      "|     4|          creative|     66000|\n",
      "|     5|financial services|    135000|\n",
      "+------+------------------+----------+\n",
      "\n",
      "[('job_id', 'bigint'), ('job_category', 'string'), ('avg_salary', 'bigint')]\n"
     ]
    }
   ],
   "source": [
    "# Create list with column names\n",
    "job_columns = ['job_id', 'job_category', 'avg_salary']\n",
    "\n",
    "# Create PySpark dataframe from query\n",
    "job_df = spark.createDataFrame(cur.execute('''SELECT * FROM cademycode_student_jobs''').fetchall(),job_columns)\n",
    "job_df.show()\n",
    "\n",
    "print(job_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af3efb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6398be4",
   "metadata": {},
   "source": [
    "The following steps will be needed to clean the data\n",
    "1. Extract the email and address for each student stores inside the _5 column dictionaries\n",
    "2. Check for null values in the dataframes\n",
    "3. Change dtype for the student_df dataframe to the following:\n",
    "    * birthdate which is a date type format to a datetime dtype\n",
    "    * Columns 'job_id', 'courses_count', 'career_path_id', 'hours_spent' from string to float\n",
    "4. Remove duplicates from all dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b9503",
   "metadata": {},
   "source": [
    "## student_df transformations\n",
    "This section will explode the json from the contact_info column into two different columns into a new dataframe named studend_address_df.\n",
    "From it,4 new columns called street, city, state and zipcode will be extracted.\n",
    "\n",
    "This will also create a new columned named year which extracts the birthdate year into a new column for easier use in further operations\n",
    "\n",
    "Aditionally, the missing values will be extracted and stored in a separate dataframe for futher analytics.\n",
    "\n",
    "Once there are no missing values, the dtypes for each column will be adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e2a67f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+\n",
      "|student_uuid|     mailing_address|       email_address|\n",
      "+------------+--------------------+--------------------+\n",
      "|           1|303 N Timber Key,...|annabelle_avery93...|\n",
      "|           2|767 Crescent Fair...| rubio6772@hmail.com|\n",
      "|           3|P.O. Box 41269, S...|hosea_dale8084@co...|\n",
      "|           4|517 SE Wintergree...|  kirk4005@hmail.com|\n",
      "|           5|18 Cinder Cliff, ...|alexander9810@hma...|\n",
      "|           6|P.O. Box 81591, T...|shavonda5863@cold...|\n",
      "|           7|P.O. Box 53471, O...|bleijenberg188@hm...|\n",
      "|           8|255 Spring Avenue...|stanford_allan805...|\n",
      "|           9|997 Dewy Apple, L...|tricia_delacruz66...|\n",
      "|          10|220 Middle Ridge,...|regenia6908@inloo...|\n",
      "|          11|818 Clear Street,...|shonda_stephanin4...|\n",
      "|          12|718 Embers Lane, ...|mcfarland1396@woo...|\n",
      "|          13|147 SW Plain, Sol...|edwardo8281@inloo...|\n",
      "|          14|P.O. Box 73926, M...|robena_padilla147...|\n",
      "|          15|868 Hazy Crossing...|tamala4408@woohoo...|\n",
      "|          16|130 Wishing Essex...|norene_dalton9509...|\n",
      "|          17|P.O. Box 93831, S...| maris5817@hmail.com|\n",
      "|          18|460 Dusty Kennedy...|vanhees6330@wooho...|\n",
      "|          19|P.O. Box 70430, L...|werner3867@coldma...|\n",
      "|          20|634 Clear Barn De...|vansteenbergen848...|\n",
      "+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new pyspark dataframe which contains the following:\n",
    "    # uuid to use in the join\n",
    "    # mailing address which is extracted from the dictionary\n",
    "    # email address which is extracted from the dictionary\n",
    "    \n",
    "student_address_df = student_df.select(\n",
    "                student_df.uuid.alias('student_uuid'),\n",
    "                f.get_json_object(student_df.contact_info, '$.mailing_address').alias(\"mailing_address\"),\n",
    "                f.get_json_object(student_df.contact_info, '$.email').alias(\"email_address\"))\n",
    "student_address_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "940a27de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the address string into separate parts\n",
    "student_address_df = student_address_df.withColumn(\"split_col\", f.split(student_address_df[\"mailing_address\"], \",\"))\n",
    "\n",
    "# Combine the first three elements of the split_col list into the \"street\" column\n",
    "student_address_df = student_address_df.withColumn(\"street\", f.array_join(f.slice(student_address_df[\"split_col\"],1,1),' '))\n",
    "\n",
    "# Extract the city, state, and zipcode from the split_col list and create separate columns\n",
    "student_address_df = student_address_df.withColumn(\"city\", f.array_join(f.slice(student_address_df[\"split_col\"],2,1),' '))\n",
    "student_address_df = student_address_df.withColumn(\"state\", f.array_join(f.slice(student_address_df[\"split_col\"],3,1),' '))\n",
    "student_address_df = student_address_df.withColumn(\"zipcode\", f.array_join(f.slice(student_address_df[\"split_col\"],4,1),' '))\n",
    "\n",
    "# Drop the split_col column\n",
    "student_address_df = student_address_df.drop(\"split_col\")\n",
    "\n",
    "# Drop the mailing address column\n",
    "student_address_df = student_address_df.drop(\"mailing_address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea8b4bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+----------------+---------------+-------+\n",
      "|student_uuid|       email_address|              street|            city|          state|zipcode|\n",
      "+------------+--------------------+--------------------+----------------+---------------+-------+\n",
      "|           1|annabelle_avery93...|    303 N Timber Key|        Irondale|      Wisconsin|  84736|\n",
      "|           2| rubio6772@hmail.com|   767 Crescent Fair|          Shoals|        Indiana|  37439|\n",
      "|           3|hosea_dale8084@co...|      P.O. Box 41269| St. Bonaventure|       Virginia|  83637|\n",
      "|           4|  kirk4005@hmail.com|517 SE Wintergree...|            Lane|       Arkansas|  82242|\n",
      "|           5|alexander9810@hma...|     18 Cinder Cliff|  Doyles borough|   Rhode Island|  73737|\n",
      "|           6|shavonda5863@cold...|      P.O. Box 81591|  Tarpon Springs|        Montana|  37057|\n",
      "|           7|bleijenberg188@hm...|      P.O. Box 53471|       Oskaloosa|       Virginia|  85274|\n",
      "|           8|stanford_allan805...|   255 Spring Avenue|     Point Baker|          Texas|  15796|\n",
      "|           9|tricia_delacruz66...|      997 Dewy Apple|    Lake Lindsey|     Washington|  78266|\n",
      "|          10|regenia6908@inloo...|    220 Middle Ridge|  Falcon Heights|     New Mexico|  46971|\n",
      "|          11|shonda_stephanin4...|    818 Clear Street|        Rockwood|          Maine|  14372|\n",
      "|          12|mcfarland1396@woo...|     718 Embers Lane|            Dale| South Carolina|  19186|\n",
      "|          13|edwardo8281@inloo...|        147 SW Plain|    Solana Beach|         Kansas|  01794|\n",
      "|          14|robena_padilla147...|      P.O. Box 73926|  McLemoresville|       Maryland|  57372|\n",
      "|          15|tamala4408@woohoo...|   868 Hazy Crossing|        Bethania|     Washington|  08205|\n",
      "|          16|norene_dalton9509...|   130 Wishing Essex|          Branch|           Ohio|  13616|\n",
      "|          17| maris5817@hmail.com|      P.O. Box 93831|  South Mountain|     Washington|  02481|\n",
      "|          18|vanhees6330@wooho...|460 Dusty Kennedy...|         McBaine|     New Jersey|  41490|\n",
      "|          19|werner3867@coldma...|      P.O. Box 70430|         Laramie|         Nevada|  56963|\n",
      "|          20|vansteenbergen848...| 634 Clear Barn Dell|          Beaman|        Georgia|  33288|\n",
      "+------------+--------------------+--------------------+----------------+---------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the resulting dataframe with the student contact information\n",
    "student_address_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c46e7ae9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+---+------+-------------+--------------+-----------+--------------------+--------------------+--------------------+---------------+-------+\n",
      "|uuid|                name| birthdate|sex|job_id|courses_count|career_path_id|hours_spent|       email_address|              street|                city|          state|zipcode|\n",
      "+----+--------------------+----------+---+------+-------------+--------------+-----------+--------------------+--------------------+--------------------+---------------+-------+\n",
      "|  26|       Doug Browning|1970-06-08|  M|   7.0|         null|           5.0|       1.92| doug7761@inlook.com|      P.O. Box 15845|              Devine|        Florida|  23097|\n",
      "|  29|      Edgardo Chavez|1946-02-12|  M|   7.0|         12.0|           5.0|      12.98|edgardo9341@wooho...|758 Green Butterf...|     Crescentvillage|          Maine|  81750|\n",
      "|  65|         Jasmine Vos|1942-08-24|  F|   5.0|         13.0|          null|       null|vos8677@coldmail.com|      P.O. Box 56722|             La Veta|        Vermont|  72685|\n",
      "| 191|         Arlen Downs|1945-10-07|  N|   5.0|         11.0|           3.0|       0.87|arlen9022@woohoo.com|      732 Lazy Apple|             Manning|          Maine|  27845|\n",
      "| 222|         Ali de Kock|1960-03-12|  M|   7.0|          4.0|           1.0|      14.87|dekock3427@coldma...|      P.O. Box 63561|              Arnett|    Mississippi|  26124|\n",
      "| 270|Hobert van der Peijl|1990-09-06|  M|   7.0|          5.0|           4.0|      22.49|hobert2230@woohoo...|      P.O. Box 36183|        Clintonville|       Michigan|  89366|\n",
      "| 293|     Simone Davidson|1958-04-22|  F|   1.0|          4.0|           4.0|      15.52|simone3163@woohoo...|    853 Amber Willow|      South Coventry|        Wyoming|  52657|\n",
      "| 243|          Elias Wise|1978-02-17|  M|   3.0|          8.0|           9.0|       30.7|  wise4765@hmail.com|   193 Embers Parade|     Fieldon village|      Louisiana|  28371|\n",
      "| 278|   Roxanne van Soest|1972-04-20|  F|   1.0|         10.0|           7.0|       6.38|roxanne3293@wooho...|652 Emerald Circl...|          Enterprise|           Utah|  97710|\n",
      "| 367|          Lexie Hale|1942-06-11|  N|   7.0|         15.0|           2.0|       1.98|  hale4143@hmail.com|       628 Mall Land|          Costa Mesa|           Iowa|  25391|\n",
      "|  19|      Dominic Werner|1973-09-29|  M|   5.0|         14.0|           8.0|      22.84|werner3867@coldma...|      P.O. Box 70430|             Laramie|         Nevada|  56963|\n",
      "|  54|      Lannie Frazier|1987-12-23|  F|   3.0|          2.0|           1.0|       4.76|frazier8553@coldm...|      769 Green Edge| Ocean Beach village| South Carolina|  63560|\n",
      "| 296|       Corie Morales|1968-12-18|  F|   4.0|          7.0|           1.0|      17.01|morales1327@wooho...|      P.O. Box 19024|           Paragonah|       Illinois|  02654|\n",
      "| 277|        Kermit Stein|1971-05-21|  M|   4.0|         12.0|           4.0|      22.68|kermit7822@woohoo...|   893 Log Highlands|         Port Graham|      Tennessee|  60703|\n",
      "| 287|       Madlyn Castro|1944-08-26|  N|   4.0|         null|          null|       null|castro932@coldmai...|      P.O. Box 26418|          Rock River|      Louisiana|  35101|\n",
      "| 348|  Marshall Palsgraaf|1994-08-12|  N|   7.0|          6.0|           9.0|       4.72|marshall_palsgraa...|     801 Clear Ridge|   Coldwater village|         Oregon|  86038|\n",
      "| 415|      Sherie Roberts|1942-12-29|  F|   2.0|         null|           5.0|       7.33|sherie_roberts869...|   472 Quaking Haven|     Trowbridge Park|         Oregon|  41315|\n",
      "| 112|      Brandie Montes|1975-03-29|  N|   5.0|         12.0|           8.0|      27.44|brandie_montes341...|      P.O. Box 35869|             Ottumwa|          Maine|  20129|\n",
      "| 113|  Andreas Williamson|1947-10-18|  M|   7.0|          5.0|           2.0|       3.36|andreas7634@coldm...|833 Silent Crossroad|      Filley village|          Maine|  83760|\n",
      "| 155|    Rosina Batenburg|1998-06-11|  N|   2.0|         12.0|           8.0|       3.17|batenburg8985@col...|         98 Red Fork|     Timberwood Park|      Tennessee|  24961|\n",
      "+----+--------------------+----------+---+------+-------------+--------------+-----------+--------------------+--------------------+--------------------+---------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the contact information dataframe to the original dataframe and drop the join column to avoid duplicity\n",
    "student_df = student_df.join(student_address_df, student_df.uuid == student_address_df.student_uuid).drop('student_uuid')\n",
    "student_df = student_df.drop('contact_info')\n",
    "student_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a854d17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+---+------+-------------+--------------+-----------+--------------------+--------------------+--------------------+---------------+-------+----------+\n",
      "|uuid|                name| birthdate|sex|job_id|courses_count|career_path_id|hours_spent|       email_address|              street|                city|          state|zipcode|birth_year|\n",
      "+----+--------------------+----------+---+------+-------------+--------------+-----------+--------------------+--------------------+--------------------+---------------+-------+----------+\n",
      "|  26|       Doug Browning|1970-06-08|  M|   7.0|         null|           5.0|       1.92| doug7761@inlook.com|      P.O. Box 15845|              Devine|        Florida|  23097|      1970|\n",
      "|  29|      Edgardo Chavez|1946-02-12|  M|   7.0|         12.0|           5.0|      12.98|edgardo9341@wooho...|758 Green Butterf...|     Crescentvillage|          Maine|  81750|      1946|\n",
      "|  65|         Jasmine Vos|1942-08-24|  F|   5.0|         13.0|          null|       null|vos8677@coldmail.com|      P.O. Box 56722|             La Veta|        Vermont|  72685|      1942|\n",
      "| 191|         Arlen Downs|1945-10-07|  N|   5.0|         11.0|           3.0|       0.87|arlen9022@woohoo.com|      732 Lazy Apple|             Manning|          Maine|  27845|      1945|\n",
      "| 222|         Ali de Kock|1960-03-12|  M|   7.0|          4.0|           1.0|      14.87|dekock3427@coldma...|      P.O. Box 63561|              Arnett|    Mississippi|  26124|      1960|\n",
      "| 270|Hobert van der Peijl|1990-09-06|  M|   7.0|          5.0|           4.0|      22.49|hobert2230@woohoo...|      P.O. Box 36183|        Clintonville|       Michigan|  89366|      1990|\n",
      "| 293|     Simone Davidson|1958-04-22|  F|   1.0|          4.0|           4.0|      15.52|simone3163@woohoo...|    853 Amber Willow|      South Coventry|        Wyoming|  52657|      1958|\n",
      "| 243|          Elias Wise|1978-02-17|  M|   3.0|          8.0|           9.0|       30.7|  wise4765@hmail.com|   193 Embers Parade|     Fieldon village|      Louisiana|  28371|      1978|\n",
      "| 278|   Roxanne van Soest|1972-04-20|  F|   1.0|         10.0|           7.0|       6.38|roxanne3293@wooho...|652 Emerald Circl...|          Enterprise|           Utah|  97710|      1972|\n",
      "| 367|          Lexie Hale|1942-06-11|  N|   7.0|         15.0|           2.0|       1.98|  hale4143@hmail.com|       628 Mall Land|          Costa Mesa|           Iowa|  25391|      1942|\n",
      "|  19|      Dominic Werner|1973-09-29|  M|   5.0|         14.0|           8.0|      22.84|werner3867@coldma...|      P.O. Box 70430|             Laramie|         Nevada|  56963|      1973|\n",
      "|  54|      Lannie Frazier|1987-12-23|  F|   3.0|          2.0|           1.0|       4.76|frazier8553@coldm...|      769 Green Edge| Ocean Beach village| South Carolina|  63560|      1987|\n",
      "| 296|       Corie Morales|1968-12-18|  F|   4.0|          7.0|           1.0|      17.01|morales1327@wooho...|      P.O. Box 19024|           Paragonah|       Illinois|  02654|      1968|\n",
      "| 277|        Kermit Stein|1971-05-21|  M|   4.0|         12.0|           4.0|      22.68|kermit7822@woohoo...|   893 Log Highlands|         Port Graham|      Tennessee|  60703|      1971|\n",
      "| 287|       Madlyn Castro|1944-08-26|  N|   4.0|         null|          null|       null|castro932@coldmai...|      P.O. Box 26418|          Rock River|      Louisiana|  35101|      1944|\n",
      "| 348|  Marshall Palsgraaf|1994-08-12|  N|   7.0|          6.0|           9.0|       4.72|marshall_palsgraa...|     801 Clear Ridge|   Coldwater village|         Oregon|  86038|      1994|\n",
      "| 415|      Sherie Roberts|1942-12-29|  F|   2.0|         null|           5.0|       7.33|sherie_roberts869...|   472 Quaking Haven|     Trowbridge Park|         Oregon|  41315|      1942|\n",
      "| 112|      Brandie Montes|1975-03-29|  N|   5.0|         12.0|           8.0|      27.44|brandie_montes341...|      P.O. Box 35869|             Ottumwa|          Maine|  20129|      1975|\n",
      "| 113|  Andreas Williamson|1947-10-18|  M|   7.0|          5.0|           2.0|       3.36|andreas7634@coldm...|833 Silent Crossroad|      Filley village|          Maine|  83760|      1947|\n",
      "| 155|    Rosina Batenburg|1998-06-11|  N|   2.0|         12.0|           8.0|       3.17|batenburg8985@col...|         98 Red Fork|     Timberwood Park|      Tennessee|  24961|      1998|\n",
      "+----+--------------------+----------+---+------+-------------+--------------+-----------+--------------------+--------------------+--------------------+---------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the year from the birthdate and store it as its own value to make it easier to access it for further analytics\n",
    "\n",
    "student_df = student_df.withColumn('birth_year', f.array_join(f.slice(f.split(student_df.birthdate, '-'),1,1),''))\n",
    "student_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71dbba16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------------+-----------------+--------------------+---------------------------+----------------------------+-------------------------+---------------------------+--------------------+------------------+-------------------+---------------------+------------------------+\n",
      "|uuid_Missing_Count|name_Missing_Count|birthdate_Missing_Count|sex_Missing_Count|job_id_Missing_Count|courses_count_Missing_Count|career_path_id_Missing_Count|hours_spent_Missing_Count|email_address_Missing_Count|street_Missing_Count|city_Missing_Count|state_Missing_Count|zipcode_Missing_Count|birth_year_Missing_Count|\n",
      "+------------------+------------------+-----------------------+-----------------+--------------------+---------------------------+----------------------------+-------------------------+---------------------------+--------------------+------------------+-------------------+---------------------+------------------------+\n",
      "|                 0|                 0|                      0|                0|                   5|                        251|                         471|                      471|                          0|                   0|                 0|                  0|                    0|                       0|\n",
      "+------------------+------------------+-----------------------+-----------------+--------------------+---------------------------+----------------------------+-------------------------+---------------------------+--------------------+------------------+-------------------+---------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the null/nan/missing values in each column\n",
    "student_df.select([f.count(f.when(f.col(col).isNull() | f.isnan(col),col)).alias(f'{col}_Missing_Count') for col in student_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36930800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------------+-----------------+--------------------+---------------------------+----------------------------+-------------------------+---------------------------+--------------------+------------------+-------------------+---------------------+------------------------+\n",
      "|uuid_Missing_Count|name_Missing_Count|birthdate_Missing_Count|sex_Missing_Count|job_id_Missing_Count|courses_count_Missing_Count|career_path_id_Missing_Count|hours_spent_Missing_Count|email_address_Missing_Count|street_Missing_Count|city_Missing_Count|state_Missing_Count|zipcode_Missing_Count|birth_year_Missing_Count|\n",
      "+------------------+------------------+-----------------------+-----------------+--------------------+---------------------------+----------------------------+-------------------------+---------------------------+--------------------+------------------+-------------------+---------------------+------------------------+\n",
      "|                 0|                 0|                      0|                0|                   5|                        251|                         471|                      471|                          0|                   0|                 0|                  0|                    0|                       0|\n",
      "+------------------+------------------+-----------------------+-----------------+--------------------+---------------------------+----------------------------+-------------------------+---------------------------+--------------------+------------------+-------------------+---------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe with the missing information instead of deleting it so it can be used to look into the reasons for missing data\n",
    "student_df_missing_info = student_df.exceptAll(student_df.dropna())\n",
    "student_df_missing_info.select([f.count(f.when(f.col(col).isNull() | f.isnan(col),col)).alias(f'{col}_Missing_Count') for col in student_df_missing_info.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cbee3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------------+-----------------+--------------------+---------------------------+----------------------------+-------------------------+---------------------------+--------------------+------------------+-------------------+---------------------+------------------------+\n",
      "|uuid_Missing_Count|name_Missing_Count|birthdate_Missing_Count|sex_Missing_Count|job_id_Missing_Count|courses_count_Missing_Count|career_path_id_Missing_Count|hours_spent_Missing_Count|email_address_Missing_Count|street_Missing_Count|city_Missing_Count|state_Missing_Count|zipcode_Missing_Count|birth_year_Missing_Count|\n",
      "+------------------+------------------+-----------------------+-----------------+--------------------+---------------------------+----------------------------+-------------------------+---------------------------+--------------------+------------------+-------------------+---------------------+------------------------+\n",
      "|                 0|                 0|                      0|                0|                   0|                          0|                           0|                        0|                          0|                   0|                 0|                  0|                    0|                       0|\n",
      "+------------------+------------------+-----------------------+-----------------+--------------------+---------------------------+----------------------------+-------------------------+---------------------------+--------------------+------------------+-------------------+---------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the null/nan/missing values from the dataframe now that they have been stored in another\n",
    "student_df = student_df.dropna()\n",
    "student_df.select([f.count(f.when(f.col(col).isNull() | f.isnan(col),col)).alias(f'{col}_Missing_Count') for col in student_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cf174ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+----------+---+--------------------+--------------------+--------------------+---------------+-------+------+-------------+--------------+-----------+\n",
      "|uuid|                name| birthdate|birth_year|sex|       email_address|              street|                city|          state|zipcode|job_id|courses_count|career_path_id|hours_spent|\n",
      "+----+--------------------+----------+----------+---+--------------------+--------------------+--------------------+---------------+-------+------+-------------+--------------+-----------+\n",
      "|  29|      Edgardo Chavez|1946-02-12|      1946|  M|edgardo9341@wooho...|758 Green Butterf...|     Crescentvillage|          Maine|  81750|     7|           12|             5|      12.98|\n",
      "| 191|         Arlen Downs|1945-10-07|      1945|  N|arlen9022@woohoo.com|      732 Lazy Apple|             Manning|          Maine|  27845|     5|           11|             3|       0.87|\n",
      "| 222|         Ali de Kock|1960-03-12|      1960|  M|dekock3427@coldma...|      P.O. Box 63561|              Arnett|    Mississippi|  26124|     7|            4|             1|      14.87|\n",
      "| 270|Hobert van der Peijl|1990-09-06|      1990|  M|hobert2230@woohoo...|      P.O. Box 36183|        Clintonville|       Michigan|  89366|     7|            5|             4|      22.49|\n",
      "| 293|     Simone Davidson|1958-04-22|      1958|  F|simone3163@woohoo...|    853 Amber Willow|      South Coventry|        Wyoming|  52657|     1|            4|             4|      15.52|\n",
      "| 243|          Elias Wise|1978-02-17|      1978|  M|  wise4765@hmail.com|   193 Embers Parade|     Fieldon village|      Louisiana|  28371|     3|            8|             9|       30.7|\n",
      "| 278|   Roxanne van Soest|1972-04-20|      1972|  F|roxanne3293@wooho...|652 Emerald Circl...|          Enterprise|           Utah|  97710|     1|           10|             7|       6.38|\n",
      "| 367|          Lexie Hale|1942-06-11|      1942|  N|  hale4143@hmail.com|       628 Mall Land|          Costa Mesa|           Iowa|  25391|     7|           15|             2|       1.98|\n",
      "|  19|      Dominic Werner|1973-09-29|      1973|  M|werner3867@coldma...|      P.O. Box 70430|             Laramie|         Nevada|  56963|     5|           14|             8|      22.84|\n",
      "|  54|      Lannie Frazier|1987-12-23|      1987|  F|frazier8553@coldm...|      769 Green Edge| Ocean Beach village| South Carolina|  63560|     3|            2|             1|       4.76|\n",
      "| 296|       Corie Morales|1968-12-18|      1968|  F|morales1327@wooho...|      P.O. Box 19024|           Paragonah|       Illinois|   2654|     4|            7|             1|      17.01|\n",
      "| 277|        Kermit Stein|1971-05-21|      1971|  M|kermit7822@woohoo...|   893 Log Highlands|         Port Graham|      Tennessee|  60703|     4|           12|             4|      22.68|\n",
      "| 348|  Marshall Palsgraaf|1994-08-12|      1994|  N|marshall_palsgraa...|     801 Clear Ridge|   Coldwater village|         Oregon|  86038|     7|            6|             9|       4.72|\n",
      "| 112|      Brandie Montes|1975-03-29|      1975|  N|brandie_montes341...|      P.O. Box 35869|             Ottumwa|          Maine|  20129|     5|           12|             8|      27.44|\n",
      "| 113|  Andreas Williamson|1947-10-18|      1947|  M|andreas7634@coldm...|833 Silent Crossroad|      Filley village|          Maine|  83760|     7|            5|             2|       3.36|\n",
      "| 155|    Rosina Batenburg|1998-06-11|      1998|  N|batenburg8985@col...|         98 Red Fork|     Timberwood Park|      Tennessee|  24961|     2|           12|             8|       3.17|\n",
      "| 167|      Jared Williams|1957-02-10|      1957|  M|williams3841@hmai...|  625 Prairie Meadow|      Norman village|          Texas|  33116|     5|            4|             7|       2.27|\n",
      "| 299|         Bettye Felt|2004-06-04|      2004|  F|bettye_felt9939@h...| 263 Lagoon Turnpike|              Agawam|           Iowa|  86366|     8|            2|             1|       5.91|\n",
      "| 385|      Grace Boogaart|1997-07-31|      1997|  F|boogaart3454@hmai...|    618 Green Divide|  La Fayette village|      Louisiana|  36663|     6|           13|             7|      19.76|\n",
      "| 237|     Adrianna Franks|1988-01-21|      1988|  F|franks1162@inlook...|616 Quaking Flat ...|           Bethlehem|  Massachusetts|  40995|     5|            8|             1|       3.82|\n",
      "+----+--------------------+----------+----------+---+--------------------+--------------------+--------------------+---------------+-------+------+-------------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('uuid', 'int'),\n",
       " ('name', 'string'),\n",
       " ('birthdate', 'date'),\n",
       " ('birth_year', 'int'),\n",
       " ('sex', 'string'),\n",
       " ('email_address', 'string'),\n",
       " ('street', 'string'),\n",
       " ('city', 'string'),\n",
       " ('state', 'string'),\n",
       " ('zipcode', 'int'),\n",
       " ('job_id', 'int'),\n",
       " ('courses_count', 'int'),\n",
       " ('career_path_id', 'int'),\n",
       " ('hours_spent', 'float')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the student_df columns and cast them to the right dtype\n",
    "sorted_columns = ['uuid', 'name', 'birthdate', \n",
    "                  'birth_year', 'sex', 'email_address',\n",
    "                  'street', 'city', 'state', 'zipcode',\n",
    "                  'job_id', 'courses_count', 'career_path_id',\n",
    "                  'hours_spent']\n",
    "column_dtypes = [IntegerType(), StringType(), DateType(),\n",
    "                 IntegerType(), StringType(), StringType(),\n",
    "                 StringType(), StringType(), StringType(), IntegerType(),\n",
    "                 IntegerType(), IntegerType(), IntegerType(),\n",
    "                 FloatType()]\n",
    "for key,value in dict(zip(sorted_columns,column_dtypes)).items():\n",
    "    student_df = student_df.withColumn(key, student_df[key].cast(value))\n",
    "student_df = student_df.select(*sorted_columns)\n",
    "student_df.show()\n",
    "student_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961e631",
   "metadata": {},
   "source": [
    "## course_df transformation\n",
    "The student_df career_path_id values will be inspected to see if these match with those in the course_df. \n",
    "\n",
    "Since course_df is a small dataframe with few dimensions, if this check is passed then there will be no need to transform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1422c336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|career_path_id|count|\n",
      "+--------------+-----+\n",
      "|             1|  437|\n",
      "|             2|  428|\n",
      "|             3|  442|\n",
      "|             4|  401|\n",
      "|             5|  446|\n",
      "|             6|  432|\n",
      "|             7|  433|\n",
      "|             8|  420|\n",
      "|             9|  417|\n",
      "|            10|  437|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_df.groupBy('career_path_id').count().orderBy('career_path_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6be28043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------------+\n",
      "|career_path_id|    career_path_name|hours_to_complete|\n",
      "+--------------+--------------------+-----------------+\n",
      "|             1|      data scientist|               20|\n",
      "|             2|       data engineer|               20|\n",
      "|             3|        data analyst|               12|\n",
      "|             4|software engineering|               25|\n",
      "|             5|    backend engineer|               18|\n",
      "|             6|   frontend engineer|               20|\n",
      "|             7|       iOS developer|               27|\n",
      "|             8|   android developer|               27|\n",
      "|             9|machine learning ...|               35|\n",
      "|            10|      ux/ui designer|               15|\n",
      "+--------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "course_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510607c8",
   "metadata": {},
   "source": [
    "## job_df\n",
    "\n",
    "Like the course_df, the jobs_df has few data entries. \n",
    "During the initial exploration there were some duplicate entries therefore they'll be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "071df347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|job_id|count|\n",
      "+------+-----+\n",
      "|     1|    1|\n",
      "|     2|    1|\n",
      "|     3|    2|\n",
      "|     4|    2|\n",
      "|     5|    2|\n",
      "|     6|    1|\n",
      "|     7|    1|\n",
      "|     8|    1|\n",
      "|     9|    1|\n",
      "|     0|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job_df.groupby('job_id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "398ee44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+----------+\n",
      "|job_id|      job_category|avg_salary|\n",
      "+------+------------------+----------+\n",
      "|     1|         analytics|     86000|\n",
      "|     2|          engineer|    101000|\n",
      "|     3|software developer|    110000|\n",
      "|     4|          creative|     66000|\n",
      "|     5|financial services|    135000|\n",
      "|     6|         education|     61000|\n",
      "|     7|                HR|     80000|\n",
      "|     8|           student|     10000|\n",
      "|     9|        healthcare|    120000|\n",
      "|     0|             other|     80000|\n",
      "+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "job_df = job_df.dropDuplicates()\n",
    "job_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3119a3",
   "metadata": {},
   "source": [
    "## Create the tables that will be loaded to the new db\n",
    "The student df columns are the following: \n",
    "['uuid', 'name', 'birthdate', \n",
    "'birth_year', 'sex', 'email_address',\n",
    "'street', 'city', 'state', 'zipcode',\n",
    "'job_id', 'courses_count', 'career_path_id',\n",
    "'hours_spent']\n",
    "\n",
    "In order to create a more managable and faster resulting dataframe, they will be split in the following way:\n",
    "\n",
    "### student_information\n",
    "1. uuid\n",
    "2. name\n",
    "3. job_id\n",
    "4. career_path_id\n",
    "\n",
    "### student_details\n",
    "1. uuid\n",
    "2. birthdate\n",
    "3. birth_year\n",
    "4. sex\n",
    "\n",
    "### student_studies\n",
    "1. uuid\n",
    "2. courses_count\n",
    "3. hours_spent\n",
    "\n",
    "### student_contact\n",
    "1. uuid\n",
    "2. email_address\n",
    "3. street\n",
    "4. city\n",
    "5. state\n",
    "6. zipcode\n",
    "\n",
    "Along the course_df and job_df. The missing_information dataframe will be fully joined and uploaded in a separate table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26b19757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tables in the new database for each dataframe\n",
    "con = sqlite3.connect(os.path.join('dev','cademycode_updated.db'))\n",
    "cur = con.cursor()\n",
    "\n",
    "# Student information\n",
    "cur.execute('''CREATE TABLE student_information (\n",
    "                uuid  INTEGER,\n",
    "                name TEXT,\n",
    "                job_id INTEGER,\n",
    "                career_path_id INTEGER)''')\n",
    "# Student details\n",
    "cur.execute('''CREATE TABLE student_details (\n",
    "                uuid INTEGER,\n",
    "                birthdate TEXT,\n",
    "                birth_year INTEGER,\n",
    "                sex VARCHAR(1))''')\n",
    "# Student studies\n",
    "cur.execute('''CREATE TABLE student_studies (\n",
    "                uuid INTEGER,\n",
    "                courses_count INTEGER,\n",
    "                hours_spent REAL)''')\n",
    "# Student contact\n",
    "cur.execute('''CREATE TABLE student_contact(\n",
    "                uuid INTEGER,\n",
    "                email_address TEXT,\n",
    "                street TEXT,\n",
    "                city TEXT,\n",
    "                state TEXT,\n",
    "                zipcode INTEGER)''')\n",
    "# Course information\n",
    "cur.execute('''CREATE TABLE course_info (\n",
    "                career_path_id INTEGER,\n",
    "                career_path_name TEXT,\n",
    "                hours_to_complete INTEGER)''')\n",
    "# Job information\n",
    "cur.execute('''CREATE TABLE job_info (\n",
    "                job_id INTEGER,\n",
    "                job_category TEXT,\n",
    "                avg_salary INTEGER)''')\n",
    "# Commit changes\n",
    "con.commit()\n",
    "# Close connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4e7f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'overwrite'\n",
    "jdbc_url = 'jdbc:sqlite:dev/cademycode_updated.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e9418eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "student_df.select(*['uuid', 'name', 'job_id', 'career_path_id'])\\\n",
    "                    .write.jdbc(url=jdbc_url, mode=mode, table='student_information')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7740432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df.select(*['uuid','birthdate','birth_year','sex'])\\\n",
    "                .write.jdbc(url=jdbc_url, mode=mode, table='student_details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3af224ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df.select(*['uuid','courses_count','hours_spent'])\\\n",
    "                .write.jdbc(url=jdbc_url, mode=mode, table='student_studies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05186984",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df.select(*['uuid','email_address','street','city','state','zipcode'])\\\n",
    "                .write.jdbc(url=jdbc_url, mode=mode, table='student_contact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53901657",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o710.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 101.0 failed 1 times, most recent failure: Lost task 5.0 in stage 101.0 (TID 374) (MiniPC executor driver): org.sqlite.SQLiteException: [SQLITE_BUSY]  The database file is locked (database is locked)\r\n\tat org.sqlite.core.DB.newSQLException(DB.java:1012)\r\n\tat org.sqlite.core.DB.newSQLException(DB.java:1024)\r\n\tat org.sqlite.core.DB.throwex(DB.java:989)\r\n\tat org.sqlite.core.DB.executeBatch(DB.java:814)\r\n\tat org.sqlite.core.CorePreparedStatement.executeBatch(CorePreparedStatement.java:64)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:713)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:867)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:65)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:757)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.sqlite.SQLiteException: [SQLITE_BUSY]  The database file is locked (database is locked)\r\n\tat org.sqlite.core.DB.newSQLException(DB.java:1012)\r\n\tat org.sqlite.core.DB.newSQLException(DB.java:1024)\r\n\tat org.sqlite.core.DB.throwex(DB.java:989)\r\n\tat org.sqlite.core.DB.executeBatch(DB.java:814)\r\n\tat org.sqlite.core.CorePreparedStatement.executeBatch(CorePreparedStatement.java:64)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:713)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8904\\3945052570.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcourse_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'career_path_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'career_path_name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'hours_to_complete'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m                 \u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjdbc_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'course_info'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\pythondata\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[1;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pythondata\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1322\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pythondata\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pythondata\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o710.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 101.0 failed 1 times, most recent failure: Lost task 5.0 in stage 101.0 (TID 374) (MiniPC executor driver): org.sqlite.SQLiteException: [SQLITE_BUSY]  The database file is locked (database is locked)\r\n\tat org.sqlite.core.DB.newSQLException(DB.java:1012)\r\n\tat org.sqlite.core.DB.newSQLException(DB.java:1024)\r\n\tat org.sqlite.core.DB.throwex(DB.java:989)\r\n\tat org.sqlite.core.DB.executeBatch(DB.java:814)\r\n\tat org.sqlite.core.CorePreparedStatement.executeBatch(CorePreparedStatement.java:64)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:713)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:867)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:65)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:757)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.sqlite.SQLiteException: [SQLITE_BUSY]  The database file is locked (database is locked)\r\n\tat org.sqlite.core.DB.newSQLException(DB.java:1012)\r\n\tat org.sqlite.core.DB.newSQLException(DB.java:1024)\r\n\tat org.sqlite.core.DB.throwex(DB.java:989)\r\n\tat org.sqlite.core.DB.executeBatch(DB.java:814)\r\n\tat org.sqlite.core.CorePreparedStatement.executeBatch(CorePreparedStatement.java:64)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:713)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "course_df.select(*['career_path_id','career_path_name','hours_to_complete'])\\\n",
    "                .write.jdbc(url=jdbc_url, mode=mode, table='course_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966fcb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df.select(*['job_id','job_category','avg_salary'])\\\n",
    "                .write.jdbc(url=jdbc_url, mode=mode, table='job_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27845d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tables in the new database for each dataframe\n",
    "con = sqlite3.connect(os.path.join('dev','cademycode_updated.db'))\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute('''SELECT * FROM student_details LIMIT 20''')\n",
    "\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
